"""
DO NOT EDIT THIS FILE
GARRETT IS USING IT TO MAKE SURE A WORKING VERSION OF HIS WORK FROM GOOGLE COLAB EXISTS SOMEWHERE ELSE

I know its gross I will clean it up later once I have a have a better version of the model set up and everything is finalized.
"""

#IMPORTS
import pandas as pd
import os
import numpy as np
import torchmetrics
import yfinance as yf
import logging
import warnings
from torch.utils.data import Dataset,DataLoader,random_split
from pathlib import Path
import re
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModel
from peft import LoraConfig, get_peft_model
from torch.utils.data import Subset
import torch
import torch.nn as nn
import torch.optim as optim
import logging
import warnings
import json
import gc
warnings.filterwarnings("ignore", module="yfinance")
torch.set_float32_matmul_precision('high')
torch.backends.cudnn.conv.fp32_precision = 'tf32'
logging.getLogger("yfinance.utils").setLevel(logging.CRITICAL)
logging.getLogger("yfinance.base").setLevel(logging.CRITICAL)

#MODEL
class LSTM_T(nn.Module):
  """
  If "google-bert/bert-large-uncased" then LoRATargetModules = ["query","key","value"]
  If "answerdotai/ModernBERT-base" then use the below code to get out the target modules

  model = AutoModel.from_pretrained("answerdotai/ModernBERT-base")
  params = []
  for name, module in model.named_modules():
      if isinstance(module, torch.nn.Linear):
          if "Wqkv" in name:
            params.append(name)
  model = FullModel(bertBaseModel = "answerdotai/ModernBERT-base",contextWindowSize = 1000 , LoRA = True,LoRARank = 2 , LoRATargetModules = params)
  """
  def __init__(self , bertBaseModel = "google-bert/bert-large-uncased",contextWindowSize = None , LoRA = False,LoRARank = None , LoRATargetModules = []):
    super(LSTM_T , self).__init__()

    #get the bert tokenizer and model
    self.bert = AutoModel.from_pretrained(bertBaseModel) #the model
    self.d = self.bert.config.hidden_size #model dim

    #set up for lora
    if LoRA:
      lora_config = LoraConfig(
        r=LoRARank,
        lora_alpha=32,
        target_modules=LoRATargetModules,
        lora_dropout=0.05,
        bias="none",
        task_type="FEATURE_EXTRACTION"
      )
      self.bert = get_peft_model(self.bert, lora_config)

    #context window
    if not contextWindowSize:
      self.l = self.bert.config.max_position_embeddings
    else:
      if contextWindowSize > self.bert.config.max_position_embeddings:
        raise Exception("context window is too large")
      self.l = contextWindowSize

    #freeze bert model weights
    for w in self.bert.parameters():
      if w.name is None or "lora_" not in w.name:
        w.requires_grad = False
      else:
        print(w.name)

    #define LSTM weights
    #Forget Gate
    self.forgetWeights = nn.Parameter(torch.rand(self.d,self.d))
    self.forgetBias = nn.Parameter(torch.rand(self.d,1))
    self.forgetTransform = nn.Parameter(torch.rand(self.l,1))
    #Input Gate
    self.longKeepWeights = nn.Parameter(torch.rand(self.d,self.d))
    self.longKeepBias = nn.Parameter(torch.rand(self.d,1))
    self.longPotentialWeights = nn.Parameter(torch.rand(self.d,self.d))
    self.longPotentialBias = nn.Parameter(torch.rand(self.d,1))
    self.inputTransform = nn.Parameter(torch.rand(self.l,1))
    #Output Gate
    self.shortKeepWeights = nn.Parameter(torch.rand(self.d,self.d))
    self.shortKeepBias = nn.Parameter(torch.rand(self.d,1))
    self.shortPotentialWeights = nn.Parameter(torch.rand(self.d,self.d))
    self.shortPotentialBias = nn.Parameter(torch.rand(self.d,1))
    self.outputTransform = nn.Parameter(torch.rand(self.l,1))

  def forward(self,x):

    #get device
    device = next(self.parameters()).device

    #cool ahhhh code go burrrr
    #get out shape
    b,d,l = x["input_ids"].shape

    #reshape as appropriate
    ids = x["input_ids"].reshape(b*d,l)

    #get out attention masks
    attnMasks = x["attention_mask"].reshape(b*d,l)

    #get out document masks
    masks = x["document_mask"]

    #move it over to the right device
    if torch.cuda.is_available():
      ids = ids.to(device)
      attnMasks = attnMasks.to(device)

    #make argument to transformer
    argument = {"input_ids":ids,"attention_mask":attnMasks}
    #Now the shape is (batch,sequence,contextwindow,modelDim)
    trans = self.bert(**argument).last_hidden_state
    trans = trans.view(b, d, l, trans.size(-1)).to("cpu")

    del ids
    del attnMasks
    gc.collect()
    torch.cuda.empty_cache()

    #hidden state and cell state
    cellState = torch.ones(trans.size(0),self.d,1,requires_grad = False,device=device)
    hiddenState = torch.ones(trans.size(0),self.d,1,requires_grad = False,device=device)

    #itterate over the sequence
    for t in range(trans.size(1)):

      startCellState = cellState.clone()
      startHiddenState = hiddenState.clone()

      doc = trans[:, t]
      if torch.cuda.is_available():
        doc = doc.to(device)
      doc = doc.permute(0, 2, 1)

      m = masks[:, t].view(trans.size(0), 1, 1)
      if torch.cuda.is_available():
        m = m.to(device)

      #forget gate
      a = (torch.tanh((self.forgetWeights @ hiddenState) + self.forgetBias))
      b = (doc @ self.forgetTransform)
      cellState = torch.mul(cellState,torch.sigmoid(a + b))

      #input gate
      inputDoc = doc @ self.inputTransform
      c = torch.tanh((self.longKeepWeights @ hiddenState) + self.longKeepBias)
      percentLong = torch.sigmoid(c + inputDoc)
      d = nn.LeakyReLU(0.2)((self.longPotentialWeights @ hiddenState) + inputDoc)
      potentialLong = torch.tanh(d + inputDoc)
      cellState += torch.mul(percentLong,potentialLong)

      #output gate
      e = torch.tanh((self.shortKeepWeights @ hiddenState) + self.shortKeepBias)
      f = doc @ self.outputTransform
      percentShort = torch.sigmoid(e+f)
      g = nn.LeakyReLU(0.2)((self.shortPotentialWeights @ cellState) + self.shortPotentialBias)
      potentialShort = torch.tanh(g)
      hiddenState = torch.mul(percentShort,potentialShort)

      #apply mask and dropout
      hiddenState = m * hiddenState + (1-m) * startHiddenState
      cellState = m * cellState + (1-m) * startCellState

    return hiddenState

class FullModel(nn.Module):

  def __init__(self , bertBaseModel = "google-bert/bert-large-uncased",contextWindowSize = None,LoRA = False,LoRARank = None,LoRATargetModules = []):

    #super and load in LSTM_T unit
    super(FullModel, self).__init__()
    self.lstmT = LSTM_T(bertBaseModel,contextWindowSize,LoRA = LoRA,LoRARank = LoRARank,LoRATargetModules = LoRATargetModules)
    self.contextWindowSize = self.lstmT.l

    #feed forward network
    self.layer1 = nn.Linear(in_features=self.lstmT.d , out_features=self.lstmT.d // 8)
    self.layer2 = nn.Linear(in_features=self.lstmT.d // 8, out_features=self.lstmT.d // 16)
    self.layer3 = nn.Linear(in_features=self.lstmT.d // 16, out_features=self.lstmT.d // 32)
    self.layer4 = nn.Linear(in_features=self.lstmT.d // 32, out_features=1)
    self.actFunc = nn.LeakyReLU(0.1)
    self.outFunc = nn.Sigmoid()
    self.dropout = nn.Dropout(p=0.3)

  def forward(self,x):
    output = self.lstmT(x)
    output = output.squeeze(-1)

    output = self.layer1(output)
    output = self.actFunc(output)
    output = self.dropout(output)
    output = self.layer2(output)
    output = self.actFunc(output)
    output = self.dropout(output)
    output = self.layer3(output)
    output = self.actFunc(output)
    output = self.dropout(output)
    output = self.layer4(output)
    output = self.outFunc(output)

    return output

#PREP FUNCTIONS
def first_business_day(year):
    return pd.Timestamp(year=year, month=1, day=1) + pd.offsets.BDay(0)

def tokenize_data(dirPath,saveDirPath,tokenizer,max_length,overwrite=False,skip = [],tokenize_batch_size = 16):
  dir = Path(dirPath)
  numFiles = sum(1 for f in dir.rglob("*") if f.is_file())
  tickers = []

  #make the new directory for savedata
  try:
    os.mkdir(saveDirPath)
  except FileExistsError:
    if not overwrite:
      print(f"Directory '{saveDirPath}' already exists.")
      return
    if overwrite:
      pass

  #get all tickers and documents from the directory
  with tqdm(total = numFiles) as pbar:
    for path in dir.iterdir():

      #get out ticker and document type
      ticker = str(path.name)[0:str(path.name).index("_")]
      docType = str(path.name)[str(path.name).index("_") + 1 : str(path.name).index(".")]

      if ticker not in tickers:
        tickers.append(ticker)

      if ticker in skip:
        pbar.update(1)
        continue

      #make the directory for the tokenized data
      try:
        os.mkdir(saveDirPath + f"{ticker}")
      except FileExistsError:
        if not overwrite:
          print(f"Directory '{saveDirPath}' already exists.")
          return
        if overwrite:
          pass

      #deal with the doc docType
      if docType == "docs":
        with open(path.resolve()) as f:
          txt = f.read()
          docs = re.split(r"\n\[EOD\]\n" , txt)
          docs.pop()
          pos = 0
          loader = DataLoader(docs , batch_size = tokenize_batch_size)
          for ds in iter(loader):
            result = tokenizer(ds,return_tensors = "pt",padding = "max_length",max_length=max_length,truncation=True)
            input_ids = result.data["input_ids"]
            attn_mask = result.data["attention_mask"]
            for j in range(len(input_ids)):
              torch.save(input_ids[j],saveDirPath + f"{ticker}/input_ids{pos}.pt")
              torch.save(attn_mask[j],saveDirPath + f"{ticker}/attention_mask{pos}.pt")
              pos += 1

      #deal with the dates docType
      if docType == "dates":
        with open(path.resolve()) as f:
          txt = f.read()
          ds = txt.split("\n")
          ds.pop()
          ds = np.array(ds)
          np.save(saveDirPath + f"{ticker}/dates.npy",ds)
          del ds
          yrs = re.findall(r"\d\d\d\d",txt)
          unq = pd.Series(yrs).unique()
          unq = np.sort(unq.astype(int))
          unq = np.array(unq)
          np.save(saveDirPath + f"{ticker}/years.npy",unq)
          del unq
      pbar.update(1)

      np.save(saveDirPath + f"tokenizedTickers.npy" , np.array(tickers))

  return tickers

def get_targets(dirPath,targetRefStock = "SPY",valid = []):
  dir = Path(dirPath)
  numDirs = sum(1 for f in dir.iterdir() if f.is_dir())
  tickers = []

  with tqdm(total = numDirs) as pbar:
    for path in dir.iterdir():

      tick = str(path.name)

      #skip over hidden dirs and files
      if "." in str(path):
        pbar.update(1)
        continue

      if tick not in valid:
        pbar.update(1)
        continue

      years = np.load(str(path) + "/years.npy")

      if tick not in tickers:
        tickers.append(tick)

      target = np.zeros(len(years))
      for i in range(len(years)):
        year = years[i]
        #start and end days for the year
        startDay = str(first_business_day(year))[0:10]
        endDay = str(first_business_day(year + 1))[0:10]

        #return over the year for the curStock
        returnPercentCur = None
        try:
          ticker = yf.Ticker(tick)
          history = ticker.history(start=startDay,end=endDay,auto_adjust=True,) #get history auto adjusted for splits and what not
          openPrice = history.iloc[0]["Open"]
          closePrice = history.iloc[-1]["Close"]
          returnPercentCur = (closePrice-openPrice)/openPrice
        except Exception as e:
          target[i] -= 1
          continue

        #return over the year for the targetRefrenceStock
        returnPercentTarget = 0 #defaults to just being positive if the refrence pos doesnt exist for that year
        try:
          ticker = yf.Ticker(targetRefStock)
          history = ticker.history(start=startDay,end=endDay,auto_adjust=True) #get history auto adjusted for splits and what not
          openPrice = history.iloc[0]["Open"]
          closePrice = history.iloc[-1]["Close"]
          returnPercentTarget = (closePrice-openPrice)/openPrice
        except:
          pass

        #if cur > target 1 else 0
        if returnPercentCur > returnPercentTarget:
          target[i] += 1

      #update the targets
      target = target.astype(int)
      np.save(str(path) + "/targets.npy",target)
      pbar.update(1)

      np.save(dirPath + f"targetedTickers.npy" , np.array(tickers))

  return tickers

def get_max_seq_len(dirPath,valid = []):
  dir = Path(dirPath)
  numDirs = sum(1 for f in dir.iterdir() if f.is_dir())

  max_seq_len = 0

  with tqdm(total = numDirs) as pbar:
    for path in dir.iterdir():

      #skip over hidden dirs
      if "." in str(path):
        pbar.update(1)
        continue

      if str(path.name) not in valid:
        pbar.update(1)
        continue

      dates = np.load(str(path) + "/dates.npy")

      if len(dates) > max_seq_len:
        max_seq_len = len(dates)

      pbar.update(1)

  return max_seq_len

def arrange_data(dirPath,max_seq_len,max_cum_length = None,valid = []):
  dir = Path(dirPath)
  numDirs = sum(1 for f in dir.iterdir() if f.is_dir())
  tickers = []

  with tqdm(total = numDirs) as pbar:
      for path in dir.iterdir():
        y = []
        maxYears = []

        #skip over hidden dirs
        if "." in str(path):
          pbar.update(1)
          continue

        if str(path.name) not in valid:
          pbar.update(1)
          continue

        ticker = str(path.name)
        dts = pd.Series(np.load(str(path) + "/dates.npy"))
        unqYrs = pd.Series(np.load(str(path) + "/years.npy"))
        tgts = pd.Series(np.load(str(path) + "/targets.npy"))
        cumIdx = np.array([])

        pos = 0
        for i in range(len(unqYrs)):
          xID = []
          xAM = []
          t = tgts.iloc[i]
          yr = unqYrs.iloc[i]
          idxs = dts[dts.str.contains(str(yr))].index.values
          cumIdx = np.concat((cumIdx,idxs)).astype(int)

          if max_cum_length != None and len(cumIdx) > max_cum_length:
            cumIdx = cumIdx[(len(cumIdx) - max_cum_length) : ]

          if t != -1: #ignores faulty years
            y.append(t)
            maxYears.append(yr)

            for idx in cumIdx:
              xID.append(torch.load(str(path) + f"/input_ids{idx}.pt"))
              xAM.append(torch.load(str(path) + f"/attention_mask{idx}.pt"))

            if len(xID) < max_seq_len:
              padMask = torch.zeros(xAM[0].shape)
              padSeq = torch.zeros(xID[0].shape)

              adds = max_seq_len - len(xID)
              while (adds > 0):
                xID.append(padSeq)
                xAM.append(padMask)
                adds -= 1

            a = np.ones(len(xID),dtype = float)
            b = np.zeros((max_seq_len - len(xID)),dtype = float)
            docMask = torch.tensor(np.concatenate((a,b)))

            xID = torch.stack(xID)
            xAM = torch.stack(xAM)

            torch.save(xID,str(path) + f"/xID{pos}.pt")
            torch.save(xAM,str(path) + f"/xAM{pos}.pt")
            torch.save(docMask,str(path) + f"/docAM{pos}.pt")
            pos += 1
        
        np.save(str(path) + f"/targets.npy" , np.array(y))
        np.save(str(path) + f"/maxYears.npy" , np.array(maxYears))
        pbar.update(1)

def validate_before_targets(dirPath):
  dir = Path(dirPath)
  numDirs = sum(1 for f in dir.iterdir() if f.is_dir())
  valid = []
  invalid = []

  with tqdm(total = numDirs) as pbar:
    for path in dir.iterdir():

      if "." in str(path):
        pbar.update(1)
        continue

      ticker = str(path.name)
      try:
        torch.load(str(dir) + f"/{ticker}/attention_mask0.pt")
        torch.load(str(dir) + f"/{ticker}/input_ids0.pt")
        np.load(str(dir) + f"/{ticker}/dates.npy")
        np.load(str(dir) + f"/{ticker}/years.npy")
        valid.append(ticker)
      except:
        invalid.append(ticker)

      pbar.update(1)

  np.save(dirPath + f"validTickers.npy" , np.array(valid))
  np.save(dirPath + f"invalidTickers.npy" , np.array(valid))
  return valid

def cleanup(dirPath):
  dir = Path(dirPath)
  numDirs = sum(1 for f in dir.iterdir() if f.is_dir())
  valid = []
  invalid = []

  with tqdm(total = numDirs) as pbar:
    for path in dir.iterdir():
      if "." in str(path):
        pbar.update(1)
        continue

      subdir = Path(str(path) + '/')
      for f in subdir.iterdir():
        if "input_ids" in str(f.name) or "attention_mask" in str(f.name):
          os.remove(f)
      pbar.update(1)
    
def prep_it(fromPath,toPath,max_length,tokenizer,overwrite,max_cum_length,tokenize_batch_size=16,skip = []):

  '''
  fromPath = "/content/drive/MyDrive/SEC-Company-Documents/testingData/"
  toPath = "/content/drive/MyDrive/SEC-Company-Documents/tokenizedTestingData/"
  max_length = 1000
  tokenizer = AutoTokenizer.from_pretrained("answerdotai/ModernBERT-base")
  overwrite = True
  max_cum_length = 500
  #fromPath,toPath,max_length,tokenizer,overwrite,max_cum_length,tokenize_batch_size=16,skip = []
  prep_it(fromPath,toPath,max_length,tokenizer,overwrite,max_cum_length,tokenize_batch_size=32,skip = [])
  '''

  tokenize_data(fromPath,toPath,tokenizer,max_length,overwrite=overwrite,tokenize_batch_size=tokenize_batch_size,skip=skip)
  validTickers = validate_before_targets(toPath)
  get_targets(toPath,valid = validTickers)
  max_seq_len = get_max_seq_len(toPath,valid = validTickers)
  arrange_data(toPath,max_seq_len,max_cum_length = max_cum_length,valid=validTickers)
  cleanup(toPath)

#CUSTOM DATASET STUFF
class DirDataset(Dataset):

  def __init__(self,dirPath,valid = []):
    self.dir = Path(dirPath)
    tickers = []
    start = []
    stop = []

    startPos = 0
    for path in self.dir.iterdir():

      #skip over hidden dirs and files
      if "." in str(path):
        continue

      if str(path.name) not in valid:
        continue

      length = len(np.load(str(path) + "/maxYears.npy")) #num samples
      tickers.append(str(path.name))
      start.append(startPos)
      stop.append((startPos + length))
      startPos += (length)

    #stop is non inclusive.
    self.posRef = []
    for ticker, s, e in zip(tickers, start, stop):
      self.posRef.extend([(ticker, i - s) for i in range(s, e)])

  def __getitem__(self,i):

    if i >= len(self):
      raise Exception(f"index {i} is out of bounds for length {len(self)}")

    ticker,indx = self.posRef[i]

    id = torch.load(str(self.dir) + f"/{ticker}/xID{indx}.pt").to(torch.long)
    am = torch.load(str(self.dir) + f"/{ticker}/xAM{indx}.pt").to(torch.float32)
    docAM = torch.load(str(self.dir) + f"/{ticker}/docAM{indx}.pt").to(torch.float32)
    target = torch.tensor(np.load(str(self.dir) + f"/{ticker}/targets.npy")).to(torch.int)

    x = {"input_ids":id,"attention_mask":am,"document_mask":docAM}
    return (x,target[indx])

  def __len__(self):
    return len(self.posRef)

  def getItemWithMaxYr(self,i):
    x,target = self[i]
    ticker,indx = self.posRef[i]
    yr = np.load(str(self.dir) + f"/{ticker}/maxYears.npy")[indx]
    return (x,target,yr)
  
  def getMaxYr(self,i):
    ticker,indx = self.posRef[i]
    yr = np.load(str(self.dir) + f"/{ticker}/maxYears.npy")[indx]
    return yr

  def getItemWithTicker(self,i):
    x,target = self[i]
    ticker,indx = self.posRef[i]
    return (x,target,ticker)
  
  def getTickYrTarget(self,i):
    ticker,indx = self.posRef[i]
    yr = np.load(str(self.dir) + f"/{ticker}/maxYears.npy")[indx]
    target = np.load(str(self.dir) + f"/{ticker}/targets.npy")[indx]
    return (ticker,yr,target)

def train_test_split(dirPath , validTickersPath , trainPercent = 0.8 , yearLimit = None):

  if yearLimit is None:
    data = DirDataset(dirPath , np.load(validTickersPath))
    trainData , testData = random_split(data , [trainPercent , 1 - trainPercent])
    return(trainData,testData)

  else:
    data = DirDataset(dirPath , np.load(validTickersPath))
    trainIndx = []
    testIndx = []
    for i in range(len(data)):
      yr = data.getMaxYr(i)
      if yr >= yearLimit:
        testIndx.append(i)
      else:
        trainIndx.append(i)

    testData = Subset(data,testIndx)
    trainData = Subset(data,trainIndx)
    return (trainData,testData)

#TRAINING FUNCTIONS
def train_model(model = None, optimizer = None, trainingData = None,savePath = None, epochs = 100, batch_size = 32,validation_percent = 0.15, waitTime = 10):
  assert (model is not None and optimizer is not None and trainingData is not None and savePath is not None)

  device = next(model.parameters()).device

  os.makedirs(os.path.dirname(savePath), exist_ok=False)
  results = {}
  with open(savePath + 'results.json', 'w') as file:
    pass

  bestValLoss = 9999999
  bestValF1 = -1
  bestTrainLoss = 9999999
  bestTrainF1 = -1

  trainF1Metric = torchmetrics.classification.BinaryF1Score()
  valF1Metric = torchmetrics.classification.BinaryF1Score()
  lossMetric = nn.BCELoss()

  train,val = random_split(trainingData,[1-validation_percent,validation_percent])
  trainLoader = DataLoader(train,batch_size = batch_size)
  valLoader = DataLoader(val,batch_size = batch_size)
  wait = waitTime
  for epoch in tqdm(range(epochs)):

    if wait == 0 :
      return

    trainLoss = 0
    trainF1 = 0
    trainN = 0
    valLoss = 0
    valF1 = 0
    valN = 0

    model.train()
    trainF1Metric.reset()
    for x,y in iter(trainLoader):
      y_hat = model(x)
      y = y.to(float)
      y_hat = y_hat.squeeze(-1).to(float)
      if torch.cuda.is_available():
        y = y.to(device)
      optimizer.zero_grad()
      loss = lossMetric(y_hat,y)
      loss.backward()
      optimizer.step()
      y = y.detach().to("cpu")
      y_hat = y_hat.detach().to("cpu")

      trainLoss += loss.item() * len(x)
      trainF1Metric.update(y_hat, y)
      trainN += len(x)
      gc.collect()
      torch.cuda.empty_cache()

    trainF1 = trainF1Metric.compute().item()

    model.eval()
    valF1Metric.reset()
    for x,y in iter(valLoader):
      with torch.no_grad():
        y_hat = model(x)
        y = y.to(float)
        y_hat = y_hat.squeeze(-1).to(float)
        if torch.cuda.is_available():
          y = y.to(device)
        y = y.detach().to("cpu")
        y_hat = y_hat.detach().to("cpu")
        loss = lossMetric(y_hat,y.to(float))
        valF1Metric.update(y_hat,y)

        valLoss += loss.item() * batch_size
        valN += len(x)
        gc.collect()
        torch.cuda.empty_cache()

      valF1 = valF1Metric.compute().item()

    trainLoss = trainLoss / trainN
    trainF1 = trainF1 / trainN
    valF1 = valF1 / valN
    valLoss = valLoss / valN

    if valLoss < bestValLoss or valF1 > bestValF1:
      if valLoss < bestValLoss:
        bestValLoss = valLoss
        torch.save(model,savePath + "validationBasedModel.bin")
        wait = waitTime + 1
      if valF1 > bestValF1:
        bestValF1 = valF1

    if trainLoss < bestTrainLoss or trainF1 > bestTrainF1:
      if trainLoss < bestTrainLoss:
        bestValLoss = trainLoss
        torch.save(model,savePath + "trainingBasedModel.bin")
        wait = waitTime + 1
      if trainF1 > bestTrainF1:
        bestTrainF1 = trainF1

    results[epoch] = {"trainLoss":trainLoss,"trainF1":trainF1,"valLoss":valLoss,"valF1":valF1}
    with open(savePath + "results.json", "w+") as outfile:
      json.dump(results, outfile)
    wait -= 1
